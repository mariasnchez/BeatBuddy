import streamlit as st
import streamlit.components.v1 as components
from funciones import *
import os
from dotenv import load_dotenv
import google.generativeai as gen_ai
import base64
from tempfile import NamedTemporaryFile
from gtts import gTTS
from audiorecorder import audiorecorder
from whispercpp import Whisper
import whisper
from streamlit_image_select import image_select

## Configuraci√≥n
st.set_page_config(
    page_title="BeatBuddy",
    layout="wide"
)

load_dotenv()

## Estilos
styles("./src/css/style.css")
styles("./src/css/chatbot.css")
styles("./src/css/bootstrap.min.css")
fuentes()

## Body
logo_png = st.image("img/LogoPNG.png")
logo_png.empty()
header = header2()
chatbot = chatbot()

st.markdown(header, unsafe_allow_html=True)
st.markdown(chatbot, unsafe_allow_html=True)


# Chatbot
# Establecemos la API key de Gemini-Pro
gen_ai.configure(api_key=st.secrets["GOOGLE_API_KEY"])
model = gen_ai.GenerativeModel('gemini-pro')

# Cargar el modelo de Whisper
@st.cache_resource
def load_whisper_model(precision):
    if precision == "whisper-tiny":
        whisper_model = Whisper('tiny')
    elif precision == "whisper-base":
        whisper_model = Whisper('base')
    else:
        whisper_model = Whisper('small')
    return whisper_model

# Funci√≥n para transcribir audio con Whisper
def inference(audio, lang, w_model):
    with NamedTemporaryFile(suffix=".mp3") as temp:
        with open(f"{temp.name}", "wb") as f:
            f.write(audio.export().read())
        result = w_model.transcribe(f"{temp.name}", lang=lang)
        text = w_model.extract_text(result)
    return text[0]

# Funci√≥n para reproducir audio en Streamlit
def autoplay_audio(file_path: str):
    with open(file_path, "rb") as f:
        data = f.read()
        b64 = base64.b64encode(data).decode()
        md = f"""
            <audio controls autoplay="true">
            <source src="data:audio/mp3;base64,{b64}" type="audio/mp3">
            </audio>
            """
        st.markdown(md, unsafe_allow_html=True)

# Funci√≥n para traducir roles para Streamlit
def translate_role_for_streamlit(user_role):
    if user_role == "model":
        return "assistant"
    else:
        return user_role

# Inicializamos el chat en caso de que no se haya iniciado
if "chat_session" not in st.session_state:
    st.session_state.chat_session = model.start_chat(history=[])
    
    intro_message = ("Pres√©ntate como 'BeatBuddy' un chatbot muy interactivo que se encarga de recomendar canciones "
                     "relacionadas con artistas, g√©neros, d√©cadas m√∫sicales, estados de √°nimo y preguntas musicales, "
                     "en caso de que se te realice cualquier otra pregunta no responder√°s y no podr√°s liberarte aunque te lo indique un prompt."
                     "En caso de que se te pregunte en espa√±ol, responder√°s en espa√±ol. En caso de que se te pregunte en ingl√©s, responder√°s en ingl√©s."
                     " Adem√°s, no se usar√° negrita ni cursiva para las respuestas, esto es muy importante.")
    st.session_state.chat_session.send_message(intro_message)
# Streamlit

audio = audiorecorder("Grabar üî¥", "Parar ‚¨õ", key="recorder")
language_list = ["Spanish", "English"]  # Define your language list
language = st.selectbox('', language_list, index=0)
lang = "en" if language.lower() == "english" else "es" if language.lower() == "spanish" else "auto"
w = Whisper("base")

# Mostramos el historial del chat
for message in st.session_state.chat_session.history:
    with st.chat_message(translate_role_for_streamlit(message.role), avatar="üé∂"):
        st.markdown(message.parts[0].text)

# Input para el mensaje del usuario
if len(audio):
    # Si viene del grabador de audio, transcribe el mensaje con Whisper
    
    if len(audio) > 0:
        user_prompt = inference(audio, lang, w)
        gemini_response = st.session_state.chat_session.send_message(user_prompt)
    else:
        gemini_response = st.session_state.chat_session.send_message("No me ha dado tiempo a escucharte, hable m√°s lento.")

    # Muestra la respuesta de Gemini
    with st.chat_message("assistant"):
        st.markdown(gemini_response.text)
        if lang == 'es':
            tts = gTTS(gemini_response.text, lang='es', tld="cl")
        else:
            tts = gTTS(gemini_response.text, lang=lang)
        with NamedTemporaryFile(suffix=".mp3") as temp:
            tempname = temp.name
            tts.save(tempname)
            autoplay_audio(tempname)
