# BeatBuddy 
<p align="center">
  <img src="/img/Logo.jpg" width=250/>
</p>

Proyecto realizado por [Christian Mart铆n D铆az](https://github.com/chrismartindiaz), [Pablo Mart铆n Trujillo](https://github.com/martintpablo) y [Mar铆a Eugenia S谩nchez S谩nchez](https://github.com/mariasnchez).


P谩gina web: 

## ndice

* [1. Descripci贸n del proyecto ](#1)
* [2. Obtenci贸n de los datos](#2)
  * [Asistente musical](#a2)
  * [Reconocimiento facial](#r2)
* [3. Exploraci贸n y visualizaci贸n de los datos](#3)
  * [Asistente musical](#a3)
  * [Reconocimiento facial](#r3)
* [4. Preparaci贸n de los datos](#4)
  * [Asistente musical](#a4)
  * [Reconocimiento facial](#r4)
* [5. Entrenamiento del modelo](#5)
  * [Asistente musical](#a5)
  * [Reconocimiento facial](#r5)
* [6. Procesamiento Lenguaje Natural (Chatbot)](#6)
* [7. Aplicaci贸n web](#7)



## 1. Descripci贸n del proyecto <a name="1"></a>
BeatBuddy tratar谩 de una aplicaci贸n de m煤sica personalizada, que servir谩 como un asistente musical capaz de proporcionar recomendaciones precisas en base a preguntas como el estado de 谩nimo, la actividad o el a帽o, entre otros par谩metros. Tambi茅n tendr谩 una parte en la que se recomendar谩 una canci贸n respecto a la emoci贸n predicha en un reconocimiento facial y otra parte de un chatbot integrado con Spotify, con la que se podr谩 tener una experiencia m谩s completa.

## 2. Obtenci贸n de datos <a name="2"></a>
### ASISTENTE MUSICAL <a name="a2"></a>
Los datos los hemos recogido mediante **scrapping** ([Ver desarrollo scrapping](/archivos/Spotipy_Scrapping.ipynb)) con la funci贸n de la [API de Spotify](https://developer.spotify.com/documentation/web-api/reference/get-audio-features) `audio-features` y tienen el siguiente significado:
- **loudness**: Se trata de un campo **float** que mide de **0,0 a 1,0** el nivel de **ac煤stica** de la canci贸n. 1,0 representa una confianza alta en que la pista es ac煤stica.

- **analysis_url**: Se trata de un campo **string** que proporciona la **URL** con los detalles de la canci贸n.

- **danceability**: Se trata de un campo **float** que mide de **0,0 a 1,0** el nivel de **bailabilidad** de una canci贸n en base a una serie de elementos como el tempo, ritmo, potencia del beat... 1,0 representa una confianza alta en que la pista es s煤per bailable.

- **duration_ms**: Campo **entero** que muestra la **duraci贸n de la canci贸n** en **milisegundos**.

- **energy**: Se trata de un campo **float** que mide de **0,0 a 1,0** el nivel de **intensidad o actividad** de la canci贸n. Generalmente las canciones con m谩s volumen o r谩pidas son m谩s energ茅ticas. 1,0 representa una confianza alta en que la pista es en茅rgica.

- **id**: ID de la canci贸n en **string**.

- **instrumentalness**: Se trata de un campo **float** que mide de **0,0 a 1,0** la **cantidad** de veces que aparecen  **melod铆as** frente a las palabras en la canci贸n. 1,0 representa una confianza alta en que la pista es instrumental.

- **key**: Se trata de un campo de tipo **entero** que se refiere a la **clave de la pista**.

- **liveness**: Se trata de un campo **float** que mide de **0,0 a 1,0** el nivel de **audiencia** de la canci贸n. 1,0 representa una confianza alta en que la pista contiene mucha audiencia.

- **loudness**: Se trata de un campo **float** que mide el nivel de **decibelios** de la canci贸n.

- **mode**: Es un campo **entero** que puede ser 1 o 0 que mide la modalidad de la escala en menor o mayor.

- **speechiness**: Se trata de un campo **float** que mide de **0,0 a 1,0** la **cantidad** de veces que aparecen  **palabras** en la canci贸n. 1,0 representa una confianza alta en que la pista es totalmente hablada.

- **tempo**: Se trata de un campo **float** que mide de el tempo estimado de una canci贸n en BPM (beats per minute).

- **time_signature**: Se trata de un campo de tipo **entero** que muestra la cantidad de beats hay en cada l铆nea de canci贸n.

- **track_href**: Un **enlace** directamente a la **API** de Spotify con los detalles de la canci贸n.

- **type**: **String** que mide el **tipo** del objeto.

- **uri**: **String** que muestra la **URI** de la canci贸n.

- **valence**: Se trata de un campo **float** que mide de **0,0 a 1,0** la **positividad** de las **letras** de cada  canci贸n. 1,0 representa una confianza alta en que la canci贸n es positiva al 100%.
- **release_date**: Es un campo tipo **string** que simplemente indica la **fecha de salida** de la canci贸n.

### RECONOCIMIENTO FACIAL <a name="r2"></a>
Los datos del dataset de kaggle *fer2013.csv* del siguiente enlace: https://www.kaggle.com/datasets/ashishpatel26/facial-expression-recognitionferchallenge

En el dataset se encuentran im谩genes de 7 expresiones diferentes: enfado, disgusto, miedo, feliz, triste, sorpresa y neutral. Para el reconocimiento nos centraremos en las im谩genes de:
- **Tristeza**
- **Felicidad**
- **Neutrales**

## 3. Exploraci贸n y visualizaci贸n de los datos <a name="3"></a>
### ASISTENTE MUSICAL <a name="a3"></a>
[Ver desarrollo](/archivos/Asistente_exploracion_visualizacion.ipynb)

Hemos le铆do el dataset, visto que no hay valores nulos, hay 137 columnas duplicadas y visualizado las correlaciones.

Gr谩fica correlaci贸n: 

<img src="/img/correlaciones_exploracion.png" width=/>


### RECONOCIMIENTO FACIAL <a name="r3"></a>
[Ver desarrollo](/archivos/Reconocimiento_exploracion_visualizacion.ipynb)

Hemos le铆do el dataset, visto las claves del diccionario (`emotion`, `pixels`, `Usage`), los tipos de emociones hay (7), el n煤mero de im谩genes que en cada emoci贸n, y la distribuci贸n de emociones.

<img src="/img/distribucion_emociones.png" width=/>

## 4. Preparaci贸n de los datos <a name="4"></a>
### ASISTENTE MUSICAL <a name="a4"></a>
[Ver desarrollo](/archivos/Asistente_preparacion_datos.ipynb)

Hemos eliminado los duplicados; utilizado t茅cnicas de *Featuring Engineering* para la eliminaci贸n de columnas innecesarias, distingir columnas, cambiar nombres, eliminaci贸n de outliers y eliminaci贸n de datos vac铆os/nulos; y mostrado las correlaciones tras el tratamiento. 

Gr谩fica correlaciones:

<img src="/img/correlaciones_preparacion.png" width=/>


### RECONOCIMIENTO FACIAL <a name="r4"></a>
[Ver desarrollo](/archivos/Reconocimiento_preparacion_datos.ipynb)

Hemos borrado la columna `Usage` ya que no la necesitamos, elegido las im谩genes con emociones `feliz`, `triste` y `neutral` y eliminado los duplicados.

## 5. Entrenamiento del modelo <a name="5"></a>
### ASISTENTE MUSICAL <a name="a5"></a>
[Ver desarrollo](/archivos/Asistente_entrenamiento.ipynb)

#### Entrenamiento para el target `mood`
Probamos con:
- SVClassifier (58% precisi贸n)
- KneighborsClassifier (79% precisi贸n)
- Regresi贸n Log铆stica (98,75% precisi贸n) **Modelo elegido**

#### Entrenamiento para el target `motivation`
Probamos con:
- SVClassifier (80% precisi贸n)
- LinearSVC (98% precisi贸n) **Modelo elegido**

Hemos a帽adido 100 canciones m谩s por d茅cada musical y los unimos a los modelos. 

### RECONOCIMIENTO FACIAL <a name="r5"></a>
[Ver desarrollo](/archivos/Reconocimiento_preparacion_datos.ipynb)

Hemos entrenado el modelo con **redes neuronales** utilizando tensorflow y keras. Ha dado un 70% de precisi贸n, pero nos quedamos con el modelo ya que las predicciones que ha fallado son las que no se identifica bien la cara que han puesto. Mientras se muestre claramente la cara, el modelo lo predice correctamente.

Aqu铆 un ejemplo con las primeras 64 im谩genes (las negras son las predicciones correctas y las naranjas las falladas): 

<img src="/img/entrenamiento.png" width=/>

## 6. Procesamiento Lenguaje Natural (Chatbot) <a name="6"></a>

## 7. Aplicaci贸n web <a name="7"></a>

Nuestra p谩gina ha sido desarrollada en Streamlit.

Se han usado tecnolog铆as como `HTML`, `CSS`, `JavaScript` y `Bootstrap`. Tambi茅n hemos usado `components.html()` que nos permite crear componentes de Streamlit, donde podemos utilizar `Animejs`, una librer铆a de JavaScript para realizar animaciones. 

Junto a todo esto, encontramos componentes de Streamlit como `streamlit-image-select`, que nos permite crear una galer铆a de im谩genes donde seleccionar una, como si de un checkbox se tratase, y `streamlit-audiorecorder`, que nos ayuda a comunicarnos con nuestro VoiceBot mediante voz.

A la hora de aplicar estilos, podemos hacerlo mediante `st.markdown(<html>, unsafe_allow_html=True)`. Para no llenar los archivos de las p谩ginas con mucho c贸digo, hemos creado un archivo llamado `funciones.py`, donde tenemos funciones que devuelven un c贸digo `html` en un string, que luego metemos en `st.markdown()`.

He aqu铆 algunos ejemplos:

Prototipo: 
