# BeatBuddy 游뱄
<p align="center">
  <img src="/img/Logo.jpg" width=250/>
</p>

Proyecto realizado por [Christian Mart칤n D칤az](https://github.com/chrismartindiaz), [Pablo Mart칤n Trujillo](https://github.com/martintpablo) y [Mar칤a Eugenia S치nchez S치nchez](https://github.com/mariasnchez).


P치gina web: 

## 칈ndice

* [1. Descripci칩n del proyecto ](#1)
* [2. Obtenci칩n de los datos](#2)
  * [Asistente musical](#a2)
  * [Reconocimiento facial](#r2)
* [3. Exploraci칩n y visualizaci칩n de los datos](#3)
  * [Asistente musical](#a3)
  * [Reconocimiento facial](#r3)
* [4. Preparaci칩n de los datos](#4)
  * [Asistente musical](#a4)
  * [Reconocimiento facial](#r4)
* [5. Entrenamiento del modelo](#5)
  * [Asistente musical](#a5)
  * [Reconocimiento facial](#r5)
* [6. Procesamiento Lenguaje Natural (Chatbot)](#6)
* [7. Aplicaci칩n web](#7)



## 1. Descripci칩n del proyecto <a name="1"></a>
BeatBuddy tratar치 de una aplicaci칩n de m칰sica personalizada, que servir치 como un asistente musical capaz de proporcionar recomendaciones precisas en base a preguntas como el estado de 치nimo, la actividad o el a침o, entre otros par치metros. Tambi칠n tendr치 una parte en la que se recomendar치 una canci칩n respecto a la emoci칩n predicha en un reconocimiento facial y otra parte de un chatbot integrado con Spotify, con la que se podr치 tener una experiencia m치s completa.

## 2. Obtenci칩n de datos <a name="2"></a>
### ASISTENTE MUSICAL <a name="a2"></a>
Los datos los hemos recogido mediante **scrapping** ([Ver desarrollo scrapping](/archivos/Spotipy_Scrapping.ipynb)) con la funci칩n de la [API de Spotify](https://developer.spotify.com/documentation/web-api/reference/get-audio-features) `audio-features` y tienen el siguiente significado:
- **loudness**: Se trata de un campo **float** que mide de **0,0 a 1,0** el nivel de **ac칰stica** de la canci칩n. 1,0 representa una confianza alta en que la pista es ac칰stica.

- **analysis_url**: Se trata de un campo **string** que proporciona la **URL** con los detalles de la canci칩n.

- **danceability**: Se trata de un campo **float** que mide de **0,0 a 1,0** el nivel de **bailabilidad** de una canci칩n en base a una serie de elementos como el tempo, ritmo, potencia del beat... 1,0 representa una confianza alta en que la pista es s칰per bailable.

- **duration_ms**: Campo **entero** que muestra la **duraci칩n de la canci칩n** en **milisegundos**.

- **energy**: Se trata de un campo **float** que mide de **0,0 a 1,0** el nivel de **intensidad o actividad** de la canci칩n. Generalmente las canciones con m치s volumen o r치pidas son m치s energ칠ticas. 1,0 representa una confianza alta en que la pista es en칠rgica.

- **id**: ID de la canci칩n en **string**.

- **instrumentalness**: Se trata de un campo **float** que mide de **0,0 a 1,0** la **cantidad** de veces que aparecen  **melod칤as** frente a las palabras en la canci칩n. 1,0 representa una confianza alta en que la pista es instrumental.

- **key**: Se trata de un campo de tipo **entero** que se refiere a la **clave de la pista**.

- **liveness**: Se trata de un campo **float** que mide de **0,0 a 1,0** el nivel de **audiencia** de la canci칩n. 1,0 representa una confianza alta en que la pista contiene mucha audiencia.

- **loudness**: Se trata de un campo **float** que mide el nivel de **decibelios** de la canci칩n.

- **mode**: Es un campo **entero** que puede ser 1 o 0 que mide la modalidad de la escala en menor o mayor.

- **speechiness**: Se trata de un campo **float** que mide de **0,0 a 1,0** la **cantidad** de veces que aparecen  **palabras** en la canci칩n. 1,0 representa una confianza alta en que la pista es totalmente hablada.

- **tempo**: Se trata de un campo **float** que mide de el tempo estimado de una canci칩n en BPM (beats per minute).

- **time_signature**: Se trata de un campo de tipo **entero** que muestra la cantidad de beats hay en cada l칤nea de canci칩n.

- **track_href**: Un **enlace** directamente a la **API** de Spotify con los detalles de la canci칩n.

- **type**: **String** que mide el **tipo** del objeto.

- **uri**: **String** que muestra la **URI** de la canci칩n.

- **valence**: Se trata de un campo **float** que mide de **0,0 a 1,0** la **positividad** de las **letras** de cada  canci칩n. 1,0 representa una confianza alta en que la canci칩n es positiva al 100%.
- **release_date**: Es un campo tipo **string** que simplemente indica la **fecha de salida** de la canci칩n.

### RECONOCIMIENTO FACIAL <a name="r2"></a>
Los datos del dataset de kaggle *fer2013.csv* del siguiente enlace: https://www.kaggle.com/datasets/ashishpatel26/facial-expression-recognitionferchallenge

En el dataset se encuentran im치genes de 7 expresiones diferentes: enfado, disgusto, miedo, feliz, triste, sorpresa y neutral. Para el reconocimiento nos centraremos en las im치genes de:
- **Tristeza**
- **Felicidad**
- **Neutrales**

## 3. Exploraci칩n y visualizaci칩n de los datos <a name="3"></a>
### ASISTENTE MUSICAL <a name="a3"></a>
[Ver desarrollo](/archivos/Asistente_exploracion_visualizacion.ipynb)

Hemos le칤do el dataset, visto que no hay valores nulos, hay 137 columnas duplicadas y visualizado las correlaciones.

Gr치fica correlaci칩n: 

<img src="/img/correlaciones_exploracion.png" width=/>


### RECONOCIMIENTO FACIAL <a name="r3"></a>
[Ver desarrollo](/archivos/Reconocimiento_exploracion_visualizacion.ipynb)

Hemos le칤do el dataset, visto las claves del diccionario (`emotion`, `pixels`, `Usage`), los tipos de emociones hay (7), el n칰mero de im치genes que en cada emoci칩n, y la distribuci칩n de emociones.

<img src="/img/distribucion_emociones.png" width=/>

## 4. Preparaci칩n de los datos <a name="4"></a>
### ASISTENTE MUSICAL <a name="a4"></a>
[Ver desarrollo](/archivos/Asistente_preparacion_datos.ipynb)

Hemos eliminado los duplicados; utilizado t칠cnicas de *Featuring Engineering* para la eliminaci칩n de columnas innecesarias, distingir columnas, cambiar nombres, eliminaci칩n de outliers y eliminaci칩n de datos vac칤os/nulos; y mostrado las correlaciones tras el tratamiento. 

Gr치fica correlaciones:

<img src="/img/correlaciones_preparacion.png" width=/>


### RECONOCIMIENTO FACIAL <a name="r4"></a>
[Ver desarrollo](/archivos/Reconocimiento_preparacion_datos.ipynb)

Hemos borrado la columna `Usage` ya que no la necesitamos, elegido las im치genes con emociones `feliz`, `triste` y `neutral` y eliminado los duplicados.

## 5. Entrenamiento del modelo <a name="5"></a>
### ASISTENTE MUSICAL <a name="a5"></a>
[Ver desarrollo](/archivos/Asistente_entrenamiento.ipynb)

#### Entrenamiento para el target `mood`
Probamos con:
- SVClassifier (58% precisi칩n)
- KneighborsClassifier (79% precisi칩n)
- Regresi칩n Log칤stica (98,75% precisi칩n) **Modelo elegido**

#### Entrenamiento para el target `motivation`
Probamos con:
- SVClassifier (80% precisi칩n)
- LinearSVC (98% precisi칩n) **Modelo elegido**

Hemos a침adido 100 canciones m치s por d칠cada musical y los unimos a los modelos. 

### RECONOCIMIENTO FACIAL <a name="r5"></a>
[Ver desarrollo](/archivos/Reconocimiento_preparacion_datos.ipynb)

Hemos entrenado el modelo con **redes neuronales** utilizando tensorflow y keras. Ha dado un 70% de precisi칩n, pero nos quedamos con el modelo ya que las predicciones que ha fallado son las que no se identifica bien la cara que han puesto. Mientras se muestre claramente la cara, el modelo lo predice correctamente.

Aqu칤 un ejemplo con las primeras 64 im치genes (las negras son las predicciones correctas y las naranjas las falladas): 

<img src="/img/entrenamiento.png" width=/>

## 6. Procesamiento Lenguaje Natural (Chatbot) <a name="6"></a>

## 7. Aplicaci칩n web <a name="7"></a>
El prototipo de la web lo dise침amos en Figma que se puede ver [aqu칤](https://www.figma.com/file/R5yJphUSRoNuWchBIt2ZaH/TFM?type=design&node-id=0-1&mode=design&t=aop7T6jbd598DzHx-0).

Nuestra p치gina ha sido desarrollada en Streamlit.

Se han usado tecnolog칤as como `HTML`, `CSS`, `JavaScript` y `Bootstrap`. Tambi칠n hemos usado `components.html()` que nos permite crear componentes de Streamlit, donde podemos utilizar `Animejs`, una librer칤a de JavaScript para realizar animaciones. 

Junto a todo esto, encontramos componentes de Streamlit como `streamlit-image-select`, que nos permite crear una galer칤a de im치genes donde seleccionar una, como si de un checkbox se tratase, y `streamlit-audiorecorder`, que nos ayuda a comunicarnos con nuestro VoiceBot mediante voz.

A la hora de aplicar estilos, podemos hacerlo mediante `st.markdown(<html>, unsafe_allow_html=True)`. Para no llenar los archivos de las p치ginas con mucho c칩digo, hemos creado un archivo llamado `funciones.py`, que importamos en las p치ginas, donde tenemos funciones que devuelven un c칩digo `html` en un string, que luego metemos en `st.markdown()` para que las cargue en la p치gina.

He aqu칤 algunos ejemplos para:
* Cargar los estilos:
  
  `funciones.py`
  
  <img src="/img/style_function.png"/>
  
  `Inicio.py` o otras p치ginas
  
  <img src="/img/cargar_style.png"/>
  
* Cargar las fuentes:
  
  `funciones.py`
  
  <img src="/img/fuentes_function.png"/>
  
  `Inicio.py` o otras p치ginas
  
  <img src="/img/cargar_fuentes.png"/>

* Cargar partes de la p치gina, como por ejemplo el header:
  
  `funciones.py`
  
  <img src="/img/header_function.png"/>
  
  `Inicio.py` o otras p치ginas
  
  <img src="/img/cargar_header.png"/>

As칤 es como estructuramos la p치gina web BeatBuddy. 
