# BeatBuddy 游뱄
<p align="center">
  <img src="/img/Logo.jpg" width=250/>
</p>

Proyecto realizado por [Christian Mart칤n D칤az](https://github.com/chrismartindiaz), [Pablo Mart칤n Trujillo](https://github.com/martintpablo) y [Mar칤a Eugenia S치nchez S치nchez](https://github.com/mariasnchez).


P치gina web: 

## 칈ndice

* [1. Descripci칩n del proyecto ](#1)
* [2. Obtenci칩n de los datos](#2)
  * [Asistente musical](#a2)
  * [Reconocimiento facial](#r2)
* [3. Exploraci칩n y visualizaci칩n de los datos](#3)
  * [Asistente musical](#a3)
  * [Reconocimiento facial](#r3)
* [4. Preparaci칩n de los datos](#4)
  * [Asistente musical](#a4)
  * [Reconocimiento facial](#r4)
* [5. Entrenamiento del modelo](#5)
  * [Asistente musical](#a5)
  * [Reconocimiento facial](#r5)
* [6. Procesamiento Lenguaje Natural (Voicebot)](#6)
* [7. Aplicaci칩n web](#7)
* [8. Porcentaje trabajo realizado](#8)



## 1. Descripci칩n del proyecto <a name="1"></a>
BeatBuddy tratar치 de una aplicaci칩n de m칰sica personalizada, que servir치 como un asistente musical capaz de proporcionar recomendaciones precisas en base a preguntas en un peque침o **formulario** como lo son el estado de 치nimo, la actividad o el a침o, entre otros par치metros. Tambi칠n tendr치 una parte en la que se recomendar치 una canci칩n respecto a la emoci칩n predicha en un **reconocimiento facial** y otra parte de un **Chat de Voz** similar a *Siri* o *Alexa* con el que se podr치 tener una experiencia m치s completa.

Hemos empleado un gran n칰mero de tecnolog칤as para la realizaci칩n del proyecto, en la siguiente imagen se muestran todas las tecnolog칤as utilizadas:

![image](https://github.com/mariasnchez/BeatBuddy/assets/146923531/1213b8c6-2b6d-4ff2-8486-d2a4c41b97ef)

## 2. Obtenci칩n de datos <a name="2"></a>
### ASISTENTE MUSICAL <a name="a2"></a>
Los datos los hemos recogido mediante **scrapping** ([Ver desarrollo scrapping](/archivos/Spotipy_Scrapping.ipynb)) con la funci칩n de la [API de Spotify](https://developer.spotify.com/documentation/web-api/reference/get-audio-features) `audio-features` y tienen el siguiente significado:
- **loudness**: Se trata de un campo **float** que mide de **0,0 a 1,0** el nivel de **ac칰stica** de la canci칩n. 1,0 representa una confianza alta en que la pista es ac칰stica.

- **analysis_url**: Se trata de un campo **string** que proporciona la **URL** con los detalles de la canci칩n.

- **danceability**: Se trata de un campo **float** que mide de **0,0 a 1,0** el nivel de **bailabilidad** de una canci칩n en base a una serie de elementos como el tempo, ritmo, potencia del beat... 1,0 representa una confianza alta en que la pista es s칰per bailable.

- **duration_ms**: Campo **entero** que muestra la **duraci칩n de la canci칩n** en **milisegundos**.

- **energy**: Se trata de un campo **float** que mide de **0,0 a 1,0** el nivel de **intensidad o actividad** de la canci칩n. Generalmente las canciones con m치s volumen o r치pidas son m치s energ칠ticas. 1,0 representa una confianza alta en que la pista es en칠rgica.

- **id**: ID de la canci칩n en **string**.

- **instrumentalness**: Se trata de un campo **float** que mide de **0,0 a 1,0** la **cantidad** de veces que aparecen  **melod칤as** frente a las palabras en la canci칩n. 1,0 representa una confianza alta en que la pista es instrumental.

- **key**: Se trata de un campo de tipo **entero** que se refiere a la **clave de la pista**.

- **liveness**: Se trata de un campo **float** que mide de **0,0 a 1,0** el nivel de **audiencia** de la canci칩n. 1,0 representa una confianza alta en que la pista contiene mucha audiencia.

- **loudness**: Se trata de un campo **float** que mide el nivel de **decibelios** de la canci칩n.

- **mode**: Es un campo **entero** que puede ser 1 o 0 que mide la modalidad de la escala en menor o mayor.

- **speechiness**: Se trata de un campo **float** que mide de **0,0 a 1,0** la **cantidad** de veces que aparecen  **palabras** en la canci칩n. 1,0 representa una confianza alta en que la pista es totalmente hablada.

- **tempo**: Se trata de un campo **float** que mide de el tempo estimado de una canci칩n en BPM (beats per minute).

- **time_signature**: Se trata de un campo de tipo **entero** que muestra la cantidad de beats hay en cada l칤nea de canci칩n.

- **track_href**: Un **enlace** directamente a la **API** de Spotify con los detalles de la canci칩n.

- **type**: **String** que mide el **tipo** del objeto.

- **uri**: **String** que muestra la **URI** de la canci칩n.

- **valence**: Se trata de un campo **float** que mide de **0,0 a 1,0** la **positividad** de las **letras** de cada  canci칩n. 1,0 representa una confianza alta en que la canci칩n es positiva al 100%.
- **release_date**: Es un campo tipo **string** que simplemente indica la **fecha de salida** de la canci칩n.

### RECONOCIMIENTO FACIAL <a name="r2"></a>
Los datos del dataset de kaggle *fer2013.csv* del siguiente enlace: https://www.kaggle.com/datasets/ashishpatel26/facial-expression-recognitionferchallenge

En el dataset se encuentran im치genes de 7 expresiones diferentes: enfado, disgusto, miedo, feliz, triste, sorpresa y neutral. Para el reconocimiento nos centraremos en las im치genes de:
- **Tristeza**
- **Felicidad**
- **Neutrales**

## 3. Exploraci칩n y visualizaci칩n de los datos <a name="3"></a>
### ASISTENTE MUSICAL <a name="a3"></a>
[Ver desarrollo](/archivos/Asistente_exploracion_visualizacion.ipynb)

Hemos le칤do el dataset, visto que no hay valores nulos, hay 137 columnas duplicadas y visualizado las correlaciones.

Gr치fica correlaci칩n: 

<img src="/img/correlaciones_exploracion.png" width=/>


### RECONOCIMIENTO FACIAL <a name="r3"></a>
[Ver desarrollo](/archivos/Reconocimiento_exploracion_visualizacion.ipynb)

Hemos le칤do el dataset, visto las claves del diccionario (`emotion`, `pixels`, `Usage`), los tipos de emociones hay (7), el n칰mero de im치genes que en cada emoci칩n, y la distribuci칩n de emociones.

<img src="/img/distribucion_emociones.png" width=/>

## 4. Preparaci칩n de los datos <a name="4"></a>
### ASISTENTE MUSICAL <a name="a4"></a>
[Ver desarrollo](/archivos/Asistente_preparacion_datos.ipynb)

Hemos eliminado los duplicados; utilizado t칠cnicas de *Featuring Engineering* para la **eliminaci칩n de columnas innecesarias**, **distingir columnas**, **cambiar nombres**, **eliminaci칩n de outliers** y **eliminaci칩n de datos vac칤os/nulos**; y mostrado las correlaciones tras el tratamiento. 

Gr치fica correlaciones **tras el tratamiento de datos**:

<img src="/img/correlaciones_preparacion.png" width=/>


### RECONOCIMIENTO FACIAL <a name="r4"></a>
[Ver desarrollo](/archivos/Reconocimiento_preparacion_datos.ipynb)

Hemos borrado la columna `Usage` ya que no la necesitamos, elegido las im치genes con emociones `feliz`, `triste` y `neutral` y eliminado los duplicados.

## 5. Entrenamiento del modelo <a name="5"></a>
### ASISTENTE MUSICAL <a name="a5"></a>
[Ver desarrollo](/archivos/Asistente_entrenamiento.ipynb)

#### Entrenamiento para el target `mood`
Probamos con:
- SVClassifier (58% precisi칩n)
- KneighborsClassifier (79% precisi칩n)
- Regresi칩n Log칤stica (98,75% precisi칩n) **Modelo elegido**

![image](https://github.com/mariasnchez/BeatBuddy/assets/146923531/1a159cf5-e6ae-457b-98eb-b40c91dca72a)

#### Entrenamiento para el target `motivation`
Probamos con:
- SVClassifier (80% precisi칩n)
- LinearSVC (98% precisi칩n) **Modelo elegido**

![image](https://github.com/mariasnchez/BeatBuddy/assets/146923531/987c532d-95eb-4578-975f-31985c9795b2)


Hemos a침adido 100 canciones m치s por d칠cada musical y los unimos a los modelos. 

### RECONOCIMIENTO FACIAL <a name="r5"></a>
[Ver desarrollo](/archivos/Reconocimiento_preparacion_datos.ipynb)

Hemos entrenado el modelo con **redes neuronales** utilizando tensorflow y keras. Ha dado aproximadamente un 73% de precisi칩n, pero nos quedamos con el modelo ya que las predicciones que ha fallado son las que no se identifica bien la cara que han puesto. Mientras se muestre claramente la cara, el modelo lo predice correctamente.

![73_Porciento_Reconocimiento](https://github.com/mariasnchez/BeatBuddy/assets/146923531/26c5060f-3f26-4ad2-b745-fcb7ff6bad05)


Aqu칤 un ejemplo con las primeras 64 im치genes (las negras son las predicciones correctas y las naranjas las falladas): 

<img src="/img/entrenamiento.png" width=/>

## 6. Procesamiento Lenguaje Natural (Voicebot) <a name="6"></a>
Para el NPL hemos decidido hacer un **voicebot**, pero no un voicebot cualquiera, sino un **asistente de voz** similares a los de **Siri** de *Apple* o a **Alexa** de *Amazon*. Para su realizaci칩n hemos usado las siguientes **librer칤as**:

* **Google Generative AI**: Es el **motor** del voicebot, aporta la funcionalidad de realizar y responder preguntas de forma interactiva con un **prompt adecuado** para que trabaje de forma correcta.
* **TempFile**: Se va a encargar de **almacenar temporalmente** las **grabaciones de voz** durante la ejecuci칩n del asistente para a posteriori, **reproducirlas al usuario**.
* **audiorecorder**: Facilita la **grabaci칩n de audios** en **Streamlit**, 칠sta librer칤a se encargar치 de **capturar y procesar** las **grabaciones de voz** de **entrada** del usuario. 
* **Whisper**: Es el encargado de una vez se ha **realizado y almacenado temporalmente el audio**, **transcribirlo** convirti칠ndolo en **texto** para que lo **entienda** el **asistente** y **genere** el **prompt**.
* **gTTS**: Una vez **generado el prompt**, lo **칰ltimo** que queda es **convertir la respuesta proporcionada** de texto **en audio**, 칠ste audio ser치 **reproducible y pausable** cuando usuario quiera. Adem치s, podr치 **cambiar la velocidad de reproducci칩n**.

**쮺칩mo hemos realizado el Voicebot?**
1. En primer lugar, hemos llamado a nuestra **API Key de Google Gemini-Pro**.

![image](https://github.com/mariasnchez/BeatBuddy/assets/146923531/aa5527ee-874b-42d1-bc22-c790174694ea)

2. A continuaci칩n, deberemos de **inicializar** el Voicebot, para ello, crearemos un **prompt inicial** que moldear치 la **estructura** del asistente y crearemos un **historial** que ir치 **almacenando respuestas**.

![image](https://github.com/mariasnchez/BeatBuddy/assets/146923531/43b5e45a-32d1-4ee2-8721-e4b99fb85c39)

3. Creamos el **bot칩n** para **grabar** el **audio** de entrada del usuario y un **selector** para el **idioma** que desee **realizar la consulta** del **usuario**. Por 칰ltimo, establecemos el modelo de **Whisper Base** puesto que es **el m치s completo** para enviar audios de unos 10 segundos. 

![image](https://github.com/mariasnchez/BeatBuddy/assets/146923531/7d6f7ac0-bba3-483b-b5c6-ca1da8275e28)

4. En caso de que el **audio** se grabe de forma **correcta** y su duraci칩n sea mayor a 0 milisegundos se **guardar치 temporalmente** para por 칰ltimo **reproducir la respuesta** generada del **prompt** con **gTTS** y **reproducirla autom치ticamente**.

![image](https://github.com/mariasnchez/BeatBuddy/assets/146923531/d33b1b17-5f7d-455a-adfb-8c8ec2ac96e8)

## 7. Aplicaci칩n web <a name="7"></a>
El prototipo de la web lo dise침amos en Figma que se puede ver [aqu칤](https://www.figma.com/file/R5yJphUSRoNuWchBIt2ZaH/TFM?type=design&node-id=0-1&mode=design&t=aop7T6jbd598DzHx-0).

Nuestra p치gina ha sido desarrollada en Streamlit.

Se han usado tecnolog칤as como `HTML`, `CSS`, `JavaScript` y `Bootstrap`. Tambi칠n hemos usado `components.html()` que nos permite crear componentes de Streamlit, donde podemos utilizar `Animejs`, una librer칤a de JavaScript para realizar animaciones. 

Junto a todo esto, encontramos componentes de Streamlit como `streamlit-image-select`, que nos permite crear una galer칤a de im치genes donde seleccionar una, como si de un checkbox se tratase, y `streamlit-audiorecorder`, que nos ayuda a comunicarnos con nuestro VoiceBot mediante voz.

A la hora de aplicar estilos, podemos hacerlo mediante `st.markdown(<html>, unsafe_allow_html=True)`. Para no llenar los archivos de las p치ginas con mucho c칩digo, hemos creado un archivo llamado `funciones.py`, que importamos en las p치ginas, donde tenemos funciones que devuelven un c칩digo `html` en un string, que luego metemos en `st.markdown()` para que las cargue en la p치gina.

He aqu칤 algunos ejemplos para:
* Cargar los estilos:
  
  `funciones.py`
  
  <img src="/img/style_function.png"/>
  
  `Inicio.py` o otras p치ginas
  
  <img src="/img/cargar_style.png"/>
  
* Cargar las fuentes:
  
  `funciones.py`
  
  <img src="/img/fuentes_function.png"/>
  
  `Inicio.py` o otras p치ginas
  
  <img src="/img/cargar_fuentes.png"/>

* Cargar partes de la p치gina, como por ejemplo el header:
  
  `funciones.py`
  
  <img src="/img/header_function.png"/>
  
  `Inicio.py` o otras p치ginas
  
  <img src="/img/cargar_header.png"/>

As칤 es como estructuramos la p치gina web BeatBuddy. 

칄ste es el resultado final de la p치gina de inicio de la web en Streamlit:

![image](https://github.com/mariasnchez/BeatBuddy/assets/146923531/2748e54b-877f-4278-86a3-cc44e564cd1d)

## 8. Porcentaje trabajo realizado <a name="8"></a>

Hemos trabajado todos por igual, no hay ninguna queja por parte de ninguno de los integrantes del grupo.

Christian: 33%  
Mar칤a: 33%  
Pablo: 33%  
